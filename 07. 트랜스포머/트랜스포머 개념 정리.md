# Word2Vec
Word2Vec의 Encoding: text를 number로 바꾸기.   
one hot encoding: thank -> [1, 0, 0], you -> [0,1,0], love -> [0,0,1]     
원 핫 인코딩은 similarity(유사성) 표현 못함     
인코딩 대신 임베딩 사용하여 유사성 표현할 수 있도록 함.     
임베딩은 인코딩보다 저차원임-> 유사성 표현 가능.    
Word2vec은 임베딩, 유사성은 이웃 단어로 판단.     
인코딩 결과가 인풋으로 들어가고 임베딩 결과가 아웃풋으로 나옴-> word2vec 결과    
output(softmax 전 결과, hidden layer의 값)가 트랜스포머의 인풋으로 들어감, input*values of the hidden layer    
feed forward, back propagation하며 hidden layer의 weight, bias가 결정됨(word2vec 모델 학습)     
man, king 비슷한 위치, queen woman 비슷한 위치 (문장 내 단어의 유사도로 판별)    
reference: https://www.youtube.com/watch?v=sY4YyacSsLc


# Transformer
트랜스포머는 인코더와 디코더로 이루어져있다.  
(토큰: 문장의 단어들을 말함.)      
- 인코더: 양방향 구조를 가짐 (토큰의 왼쪽, 오른쪽 단어를 참조함)    
- 디코더: 단방향 구조를 가짐 (무조건 왼쪽-> 오른쪽으로 감, 토큰의 왼쪽만 참조함.)
- transformer: 병렬화, 일을 한번에 처리함. RNN은 순차적으로 계산하며 입력된 단어 인코딩, 트랜스포머는 한방에 처리.
- attention 모델을 적용, rnn이 아니도록.
- positional encoding 하여 인풋에 넣어줌 (위치정보 포함)
  
I like playing cards
*encoder: if we are looking at the word 'like', we can get informations from I and playing       
*decoder: if we are looking at the word 'like' we can get informations from 'I' only(unidirectional)

